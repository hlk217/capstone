{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from data_cleaning import *  # call our own function from a python file\n",
    "from pretrain_model import * # call our own function from a python file\n",
    "from IPython.display import Image  # display images\n",
    "\n",
    "# pd.set_option('display.max_colwidth', -1) # displace all text within a col "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple_preprocess() function in word2vec will convert a document into a list of lowercase tokens, punctuation and numbers will be removed. However, additional pre-processing is needed to remove stop words before NLP analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) replace t-shirt/T-shirt with tshirt/Tshirt to be counted as one token\n",
    "\n",
    "2) materials: \n",
    "convert (Viscose 100%) to (Viscose100%, Viscose)\n",
    "convert (100% Viscose) to (Viscose100%, Viscose)\n",
    "\n",
    "3) remove brands: \n",
    "PrettyLittleThing, ASOS DESIGN, 'ASOS', \"YAS\", \"Ditsy\", \"Noisy\", \"May\", \"Ted\",\"Baker\", \"River\",\"Island\", \"Karen\",\"Scott\", \"PrettyLittleThing\", \"Roxy\", \"DESIGN\", \"Chi\", \"Alfani\", \"Boohoo\", \"Sofie\", \"Schnoor\", \"Ellesse\", \"Jeannie\", \"TFNC\", \"Sacred\", \"Hawk\", \"Urban\", \"Bliss\", \"Puma\", \"adidas\", \"Stella\" etc.\n",
    "\n",
    "4) remove words: \n",
    "cm\n",
    "size,\n",
    "‘Web ID:’ \n",
    "Approx. model height is 5'10\" and she is wearing a size 4/S\n",
    "Made In USA Made In USA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('all.csv').drop('Unnamed: 0', axis=1).drop_duplicates('alltext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_cleaning(df) # own function from a python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[1128,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: no need to hold out the test data, as data is unlabelled (NLP is unsupervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: pretrained model for word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### used pretrained model, which uses 100k or 1M words to develop each word vector\n",
    "#### did not use our own word vector because our sample size is too small - we only around 50k unique words in our webscraped description) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pretrain_model = loadGloveModel('glove.42B.300d.txt') # own function from a python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_similar = find_similar(df, 100, pretrain_model, count = 20)  # own function from a python file\n",
    "pretrain_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pretrain_similar)):\n",
    "    display(Image(filename = './product_images/'+str(pretrain_similar[i][0])+'.jpg', width=200, height=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MyDocs reading from a data frame\n",
    "class MyDocs(object):\n",
    "    def __iter__(self):\n",
    "        for i in range(df.shape[0]):\n",
    "            yield TaggedDocument(words=simple_preprocess(df.iloc[i,0]), tags=['%s' % df.iloc[i,-1]])  # generator \n",
    "                    # the \"tag\" for each item description will be the item itself (image id), since we want to find which description is most similar to another item description\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note: no pretrained model for doc to vec because each document is unique per training set. word to vec can be generalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import multiprocessing\n",
    "import os\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "if not os.path.exists('models/doc2vec.model'):\n",
    "    print(\"start traing doc2vec model...\")\n",
    "    documents = MyDocs()                                                          # workers=cores means number of CPUs\n",
    "    doc2vec_model = Doc2Vec(dm=1, dbow_words=1, vector_size=200, window=3, min_count=2, workers=cores)  # dm=1, dbow_words=1 means train two models and take the average of distributed memory method, and distributed bad of words method\n",
    "    doc2vec_model.build_vocab(documents)\n",
    "    doc2vec_model.train(documents, total_examples=doc2vec_model.corpus_count, epochs=100) \n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "        doc2vec_model.save('models/doc2vec.model')\n",
    "    else:\n",
    "        doc2vec_model.save('models/doc2vec.model')\n",
    "else:\n",
    "    doc2vec_model = Doc2Vec.load('models/doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = MyDocs()  # just to show what was feed into the doc2vec_model.train()\n",
    "list(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to display similar images to the one you specify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_d2v(model, item, n): \n",
    "    doc2vec_model = Doc2Vec.load(model)\n",
    "    print(doc2vec_model.docvecs.most_similar(item, topn=n))\n",
    "    d2v_similar = doc2vec_model.docvecs.most_similar(item, topn=n)\n",
    "\n",
    "    # show img\n",
    "    display(Image(filename = './product_images/'+item+'.jpg', width=200, height=200))\n",
    "    for i in range(len(d2v_similar)):\n",
    "        display(Image(filename = './product_images/'+str(d2v_similar[i][0])+'.jpg', width=200, height=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm = 1, dbow_words=1\n",
    "\n",
    "load_img_d2v('models/doc2vec.model', '100', 20)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### product similarity based on image analysis (to compare against doc2vec similar images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_score = pd.read_csv('product_similarity_matrix.csv')\n",
    "img_score = img_score.drop('Unnamed: 0', axis=1)\n",
    "img_score.index = img_score.columns.values.tolist() # change row names to img name\n",
    "img_score.to_pickle('product_similarity_matrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_score = pd.read_pickle(\"product_similarity_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_score.shape\n",
    "img_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "item = 10609\n",
    "img_similar = img_score['product_images/'+str(item)+'.jpg'].sort_values(ascending =False)[:10].reset_index()\n",
    "\n",
    "print(img_similar)\n",
    "\n",
    "# show img\n",
    "for i in range(len(img_similar)):\n",
    "    display(Image(filename = './'+img_similar['index'][i], width=200, height=200))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmean clustering with doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df.website =='ASOS'] # only did ASOS because we want a direct comparison with the TSNE plot generated from LDA (for LDA we only perfromed style clustering on ASOS items) \n",
    "df1 = df1.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MyDocs reading from a data frame\n",
    "class MyDocs(object):\n",
    "    def __iter__(self):\n",
    "        for i in range(df1.shape[0]):\n",
    "            yield TaggedDocument(words=simple_preprocess(df1.iloc[i,1]), tags=['%s' % df1.iloc[i,-1]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists('models/doc2vec.model'):\n",
    "    print(\"start traing doc2vec model...\")\n",
    "    documents = MyDocs()                                                          \n",
    "    doc2vec_model = Doc2Vec(dm=1, dbow_words=1, vector_size=200, window=3, min_count=2, workers=cores)\n",
    "    doc2vec_model.build_vocab(documents)\n",
    "    doc2vec_model.train(documents, total_examples=doc2vec_model.corpus_count, epochs=100) \n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "        doc2vec_model.save('models/doc2vec.model')\n",
    "    else:\n",
    "        doc2vec_model.save('models/doc2vec.model')\n",
    "else:\n",
    "    doc2vec_model = Doc2Vec.load('models/doc2vec.model')\n",
    "    \n",
    "    # no pretrained model for doc to vec because each document is unique per training set. word to vec can be generalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use 6 clusters to compare against TSNE from LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model.docvecs[6895] # vector representing index 6895 (or item 10214)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "NUM_CLUSTERS = 6\n",
    "vectors = []\n",
    "\n",
    "#model = Doc2Vec.load('models/doc2vec_dm0_word0_vec200_win3_min2_epoch100.model')\n",
    "for i in range(len(df1)):\n",
    "    vectors.append(doc2vec_model.docvecs[i])\n",
    "\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25) # kmean with cosine disctance\n",
    "assigned_clusters = kclusterer.cluster(vectors, assign_clusters=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(assigned_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['cluster'] = assigned_clusters # cluster for each description \n",
    "df1['vectors'] = vectors  # document vector for each description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vect_list = df1['vectors'].apply(lambda x: list(x)).tolist()  # convert document vector into a nested list so we can feed it into the TSNE model\n",
    "vect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2)  # 2 dimensions \n",
    "X_tsne = tsne.fit_transform(vect_list) # TSNE reduce the document vector from 200 dimensions down to 2 dimensions\n",
    "X_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['X_tsne'] =X_tsne[:, 0] # plot one of the dimension on X-axis\n",
    "df1['Y_tsne'] =X_tsne[:, 1] # plot another dimension on Y-axis\n",
    "\n",
    "cluster_colors = {0: 'blue', 1: 'green', 2: 'yellow', 3: 'red', 4: 'skyblue', 5:'salmon', 6:'orange', 7:'maroon', 8:'crimson', 9:'black', 10:'gray'}\n",
    "\n",
    "df1['colors'] = df1['cluster'].apply(lambda l: cluster_colors[l]) # assign color to each cluster (for graphing purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook, save  # for TSNE graph\n",
    "from bokeh.models import HoverTool, value, LabelSet, Legend, ColumnDataSource\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ColumnDataSource(dict(     # for plotting\n",
    "    x=df1['X_tsne'],\n",
    "    y=df1['Y_tsne'],\n",
    "    color=df1['colors'],\n",
    "    topic_cluster= df1['cluster'],\n",
    "    label=df1['cluster'],\n",
    "    title= df1['id'],\n",
    "    website = df1['website']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'T-SNE visualization of topics'\n",
    "\n",
    "plot_lda = figure(plot_width=1000, plot_height=600,\n",
    "                     title=title, tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "                     x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "plot_lda.scatter(x='x', y='y', legend='label', source=source, color='color', alpha=0.8, size=10)\n",
    "\n",
    "# hover tools\n",
    "hover = plot_lda.select(dict(type=HoverTool))\n",
    "hover.tooltips = {\"content\": \"Prod_name: @title, website: @website - Topic: @topic_cluster \"}\n",
    "plot_lda.legend.location = \"top_left\"\n",
    "\n",
    "show(plot_lda)\n",
    "\n",
    "#save the plot\n",
    "save(plot_lda, '{}.html'.format(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
